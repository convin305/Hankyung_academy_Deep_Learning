{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB 영화 리뷰 감성 분석\r\n",
    "- 테스트 데이터 25000건 중 10000건은 Validation 데이터로 활용\r\n",
    "- Conv1D  \r\n",
    "        컨볼루션 : 특징을 추출해내는 것, 이미지의 경우 2차원이니 2d, 텍스트는 1차원이니 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "seed = 2021\r\n",
    "np.random.seed(seed)\r\n",
    "tf.random.set_seed(seed)\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\r\n",
    "(X_train, y_train), (X_test,y_test) = imdb.load_data(num_words=None)\r\n",
    "\r\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict = {}\r\n",
    "\r\n",
    "for key, value in imdb.get_word_index().items():\r\n",
    "    index_dict[value] = key\r\n",
    "\r\n",
    "len(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the as there in at by br of sure many br of proving no only women was than doesn't as you never of hat night that with ignored they bad out superman plays of how star so stories film comes defense date of wide they don't do that had with of hollywood br of my seeing fan this of pop out body shots in having because cause it's stick passing first were enjoys for from look seven sense from me superimposition die in character as cuban issues but is you that isn't one song just is him less are strongly not are you that different just even by this of you there is eight when it part are film's love film's 80's was big also light don't wrangling as it in character looked cinematography so stories is far br man acting\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(index_dict[s] for s in X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영화평 최대 길이 :  2494\n",
      "영화평 평균 길이 :  238.71364\n"
     ]
    }
   ],
   "source": [
    "print('영화평 최대 길이 : ', max(len(l) for l in X_train))\r\n",
    "print('영화평 평균 길이 : ', sum(map(len,X_train)) / len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D와 LSTM으로 IMDB 리뷰 감성 분류\r\n",
    "- 단어 빈도수 : 5,000(총단어수는 88,584)  \r\n",
    "- 문장의 단어수 : 500단어 (최대 : 2,494)  \r\n",
    "- Test data 중 10000개는 검증데이터로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.models import Sequential, load_model\r\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, Dropout, MaxPooling1D\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도수 : 5,000\r\n",
    "(X_train, y_train), (X_test,y_test) = imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 단어수 : 500단어 \r\n",
    "max_len = 500\r\n",
    "\r\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\r\n",
    "X_test = pad_sequences(X_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 500), (10000, 500), (15000,), (10000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test데이터 중 만개는 검증 데이터로\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, stratify=y_test, test_size=0.4, random_state=seed)\r\n",
    "\r\n",
    "X_test.shape, X_val.shape, y_test.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의 / 설정 /학습 / 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 120)         600000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 120)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          38464     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 60)                30000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 668,525\n",
      "Trainable params: 668,525\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\r\n",
    "    Embedding(5000,120),\r\n",
    "    Dropout(0.5),\r\n",
    "    Conv1D(64, 5, activation='relu'),\r\n",
    "    MaxPooling1D(pool_size=4),\r\n",
    "    LSTM(60),\r\n",
    "    Dense(1, activation='sigmoid')\r\n",
    "])\r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\r\n",
    "    optimizer='adam',\r\n",
    "    loss = 'binary_crossentropy',metrics=['accuracy']\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback 함수\r\n",
    "model_path = 'model/IMDB_Conv1d_lstm_best.h5'\r\n",
    "checkpointer = ModelCheckpoint(\r\n",
    "    model_path, monitor='val_loss', verbose=1, save_best_only=True\r\n",
    ")\r\n",
    "early_stopping = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.28942, saving model to model\\IMDB_Conv1d_lstm_best.h5\n",
      "250/250 - 59s - loss: 0.4114 - accuracy: 0.7947 - val_loss: 0.2894 - val_accuracy: 0.8837\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.28942 to 0.26943, saving model to model\\IMDB_Conv1d_lstm_best.h5\n",
      "250/250 - 55s - loss: 0.2365 - accuracy: 0.9086 - val_loss: 0.2694 - val_accuracy: 0.8907\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26943\n",
      "250/250 - 55s - loss: 0.1944 - accuracy: 0.9250 - val_loss: 0.2784 - val_accuracy: 0.8836\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26943\n",
      "250/250 - 55s - loss: 0.1577 - accuracy: 0.9414 - val_loss: 0.3079 - val_accuracy: 0.8792\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26943\n",
      "250/250 - 52s - loss: 0.1328 - accuracy: 0.9524 - val_loss: 0.2997 - val_accuracy: 0.8798\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26943\n",
      "250/250 - 49s - loss: 0.1101 - accuracy: 0.9624 - val_loss: 0.3247 - val_accuracy: 0.8794\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26943\n",
      "250/250 - 50s - loss: 0.0890 - accuracy: 0.9698 - val_loss: 0.3385 - val_accuracy: 0.8755\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\r\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size = 100, validation_data = (X_val, y_val), verbose=2, callbacks=[checkpointer, early_stopping] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 9s 19ms/step - loss: 0.2695 - accuracy: 0.8879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2695341408252716, 0.8879333138465881]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = load_model(model_path)\r\n",
    "best_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM의 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 120)         600000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 120)               115680    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 121       \n",
      "=================================================================\n",
      "Total params: 715,801\n",
      "Trainable params: 715,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\r\n",
    "    Embedding(5000,120),\r\n",
    "    LSTM(120),\r\n",
    "    Dense(1, activation='sigmoid')\r\n",
    "])\r\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\r\n",
    "    optimizer='adam',\r\n",
    "    loss = 'binary_crossentropy',metrics=['accuracy']\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback 함수\r\n",
    "model_path = 'model/IMDB_lstm_best.h5'\r\n",
    "checkpointer = ModelCheckpoint(\r\n",
    "    model_path, monitor='val_loss', verbose=1, save_best_only=True\r\n",
    ")\r\n",
    "early_stopping = EarlyStopping(patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30666, saving model to model\\IMDB_lstm_best.h5\n",
      "250/250 - 284s - loss: 0.4150 - accuracy: 0.8045 - val_loss: 0.3067 - val_accuracy: 0.8744\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.30666\n",
      "250/250 - 273s - loss: 0.2710 - accuracy: 0.8908 - val_loss: 0.3170 - val_accuracy: 0.8665\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.30666\n",
      "250/250 - 290s - loss: 0.2248 - accuracy: 0.9128 - val_loss: 0.3182 - val_accuracy: 0.8671\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.30666\n",
      "250/250 - 305s - loss: 0.1979 - accuracy: 0.9240 - val_loss: 0.3351 - val_accuracy: 0.8692\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30666\n",
      "250/250 - 303s - loss: 0.1793 - accuracy: 0.9314 - val_loss: 0.3707 - val_accuracy: 0.8645\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.30666\n",
      "250/250 - 307s - loss: 0.1415 - accuracy: 0.9476 - val_loss: 0.3887 - val_accuracy: 0.8579\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\r\n",
    "history2 = model2.fit(X_train, y_train, epochs=50, batch_size = 100, validation_data = (X_val, y_val), verbose=2, callbacks=[checkpointer, early_stopping] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 48s 103ms/step - loss: 0.3049 - accuracy: 0.8735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3048918545246124, 0.8734666705131531]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = load_model(model_path)\r\n",
    "best_model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c1eae21719a0790335dcb83aad72b63b602cfe5cdb2bda0f60bc11d4f154e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}